---
title: "Final"
author: "Group7"
date: "2025-11-30"
output: pdf_document
---

```{r setup, include=FALSE}
library(randomForest)
library(caret)
library(dplyr)
library(pROC)
library(ggplot2)
library(MASS)
```

```{r}
df = read.csv("diabetes_binary_5050split_health_indicators_BRFSS2015.csv")
# summary(df)

df$Diabetes_binary = factor(df$Diabetes_binary,
                            levels = c(0,1),
                            labels = c("NoDiabetes", "Diabetes"))
# summary(df)

set.seed(123)

# 70% as train, 30% as test
train_index = createDataPartition(df$Diabetes_binary, p = 0.70, list = FALSE)
train_data  = df[train_index, ]
test_data   = df[-train_index, ]

n_total = nrow(df)
cat("Training set size:", nrow(train_data), "\n")
cat("Testing set size:", nrow(test_data), "\n")
cat("train % =", nrow(train_data) / n_total, "\n")
cat("test  % =", nrow(test_data) / n_total, "\n")

# ==========================================================================
# Logistic Regression
# ==========================================================================

# 5-fold cross-validation on training data

set.seed(123)
folds = createFolds(train_data$Diabetes_binary, k = 5, returnTrain = FALSE)

cv_full   = data.frame(fold = 1:5, Accuracy = NA, AUC = NA)
cv_red    = data.frame(fold = 1:5, Accuracy = NA, AUC = NA)

for (i in 1:5) {
  idx_valid = folds[[i]]
  cv_train  = train_data[-idx_valid, ]
  cv_valid  = train_data[idx_valid, ]

  # full model
  fit_full = glm(
    Diabetes_binary ~ .,
    data   = cv_train,
    family = binomial
  )

  prob_full = predict(fit_full, newdata = cv_valid, type = "response")
  pred_full = ifelse(prob_full >= 0.5, "Diabetes", "NoDiabetes")
  pred_full = factor(pred_full, levels = levels(train_data$Diabetes_binary))

  cm_full   = confusionMatrix(pred_full, cv_valid$Diabetes_binary)
  cv_full$Accuracy[i] = cm_full$overall["Accuracy"]

  roc_full  = roc(as.numeric(cv_valid$Diabetes_binary == "Diabetes"), prob_full)
  cv_full$AUC[i] = as.numeric(auc(roc_full))

  # reduced model 
  fit_red = glm(
    Diabetes_binary ~ HighBP + HighChol + CholCheck + BMI +
      Stroke + HeartDiseaseorAttack + HvyAlcoholConsump +
      GenHlth + MentHlth + PhysHlth + DiffWalk +
      Sex + Age + Education + Income,
    data   = cv_train,
    family = binomial
  )

  prob_red = predict(fit_red, newdata = cv_valid, type = "response")
  pred_red = ifelse(prob_red >= 0.5, "Diabetes", "NoDiabetes")
  pred_red = factor(pred_red, levels = levels(train_data$Diabetes_binary))

  cm_red   = confusionMatrix(pred_red, cv_valid$Diabetes_binary)
  cv_red$Accuracy[i] = cm_red$overall["Accuracy"]

  roc_red  = roc(as.numeric(cv_valid$Diabetes_binary == "Diabetes"), prob_red)
  cv_red$AUC[i] = as.numeric(auc(roc_red))
}

# 5-fold CV average result
colMeans(cv_full[, c("Accuracy", "AUC")])
colMeans(cv_red[,  c("Accuracy", "AUC")])

# fit the model on entire train_data

# full model
logit_model = glm(
  Diabetes_binary ~ .,
  data   = train_data,
  family = binomial
)
summary(logit_model)

# reduced model
reduced_model = glm(
  Diabetes_binary ~ HighBP + HighChol + CholCheck + BMI +
    Stroke + HeartDiseaseorAttack + HvyAlcoholConsump +
    GenHlth + MentHlth + PhysHlth + DiffWalk +
    Sex + Age + Education + Income,
  data   = train_data,
  family = binomial
)
summary(reduced_model)

# test set prediction（full + reduced）

# full model on test
test_prob_full = predict(logit_model, newdata = test_data, type = "response")
test_pred_full = ifelse(test_prob_full >= 0.5, "Diabetes", "NoDiabetes")
test_pred_full = factor(test_pred_full, levels = levels(test_data$Diabetes_binary))

cm_test_full = confusionMatrix(test_pred_full, test_data$Diabetes_binary)
cm_test_full

# reduced model on test
test_prob_red = predict(reduced_model, newdata = test_data, type = "response")
test_pred_red = ifelse(test_prob_red >= 0.5, "Diabetes", "NoDiabetes")
test_pred_red = factor(test_pred_red, levels = levels(test_data$Diabetes_binary))

cm_test_red = confusionMatrix(test_pred_red, test_data$Diabetes_binary)
cm_test_red

# ROC + AUC（test set）

test_y = as.numeric(test_data$Diabetes_binary == "Diabetes")

roc_full_test = roc(test_y, test_prob_full)
roc_red_test  = roc(test_y, test_prob_red)

auc_full_test = auc(roc_full_test)
auc_red_test  = auc(roc_red_test)

auc_full_test
auc_red_test

# plot ROC curve
plot(roc_red_test, col = "blue", main = "ROC Comparison on Test Set (70/30 Split)")
plot(roc_full_test, col = "red", add = TRUE)
legend("bottomright",
       legend = c("Reduced model", "Full model"),
       col    = c("blue", "red"),
       lwd    = 2)

# Likelihood Ratio Test (Nested Model Comparison)
anova(reduced_model, logit_model, test = "Chisq")

# Additional Model Selection Metrics: PRESS, Cp, AIC, BIC
# PRESS (Prediction Error Sum of Squares)
calc_press = function(model) {
  h = hatvalues(model)
  r = residuals(model)
  press = sum((r / (1 - h))^2)
  return(press)
}

PRESS_full = calc_press(logit_model)
PRESS_red  = calc_press(reduced_model)

cat("PRESS (full model):", PRESS_full, "\n")
cat("PRESS (reduced model):", PRESS_red, "\n\n")


# Mallows' Cp (use deviance as logistic RSS)
RSS_full = logit_model$deviance
RSS_red  = reduced_model$deviance

n  = nrow(train_data)

p_full = length(coef(logit_model))
p_red  = length(coef(reduced_model))

sigma2_full = RSS_full / (n - p_full)

Cp_full = RSS_full / sigma2_full - (n - 2*p_full)
Cp_red  = RSS_red  / sigma2_full - (n - 2*p_red)

cat("Cp (full model):", Cp_full, "\n")
cat("Cp (reduced model):", Cp_red, "\n\n")


# AIC & BIC
AIC_full = AIC(logit_model)
BIC_full = BIC(logit_model)

AIC_red = AIC(reduced_model)
BIC_red = BIC(reduced_model)

cat("AIC (full model):", AIC_full, "\n")
cat("AIC (reduced model):", AIC_red, "\n\n")

cat("BIC (full model):", BIC_full, "\n")
cat("BIC (reduced model):", BIC_red, "\n\n")
```

#Random Forest
```{r}
library(pROC)

set.seed(123)
folds_rf <- createFolds(train_data$Diabetes_binary, k = 5, returnTrain = FALSE)

roc_list <- list()

for (i in 1:5) {
  idx_valid <- folds_rf[[i]]
  cv_train  <- train_data[-idx_valid, ]
  cv_valid  <- train_data[idx_valid, ]
  
  rf_model <- randomForest(
    reduced_formula,
    data = cv_train,
    ntree = 500,
    mtry = floor(sqrt(15)),
    importance = TRUE
  )
  
  rf_prob <- predict(rf_model, cv_valid, type = "prob")[, "Diabetes"]
  
  # Store ROC object
  roc_list[[i]] <- roc(cv_valid$Diabetes_binary, rf_prob)
}

# Plot all ROC curves together
plot(roc_list[[1]], col = "red", main = "ROC Curves - 5-Fold CV (Random Forest)")
for (i in 2:5) {
  plot(roc_list[[i]], col = i, add = TRUE)
}

legend("bottomright",
       legend = paste("Fold", 1:5, "AUC =", round(sapply(roc_list, auc), 3)),
       col = 1:5,
       lwd = 2)

# Average ROC curve (pooled predictions across folds)
all_probs <- unlist(lapply(1:5, function(i) predict(
  randomForest(reduced_formula, data = train_data[-folds_rf[[i]], ], ntree = 500, mtry = floor(sqrt(15))),
  train_data[folds_rf[[i]], ], type = "prob")[, "Diabetes"]
))
all_labels <- unlist(lapply(1:5, function(i) train_data[folds_rf[[i]], "Diabetes_binary"]))

roc_avg <- roc(all_labels, all_probs)
plot(roc_avg, col = "black", lwd = 3, add = TRUE)
legend("bottomleft", legend = paste("Average ROC (AUC =", round(auc(roc_avg), 3), ")"),
       col = "black", lwd = 3)

```


```{r}
cv_summary <- colMeans(cv_rf[, c("Accuracy","Precision","Recall","F1","AUC")])
print(cv_summary)

# Evaluate on the test set (final model) 
# Fit RF on full training data
rf_final <- randomForest(
  reduced_formula,
  data = train_data,
  ntree = 500,
  mtry = floor(sqrt(15)),
  importance = TRUE
)

# Predictions on test set
rf_test_pred <- predict(rf_final, test_data)
rf_test_pred <- factor(rf_test_pred, levels = c("NoDiabetes","Diabetes"))
rf_test_prob <- predict(rf_final, test_data, type = "prob")[, "Diabetes"]

# Confusion matrix on test set
cm_test <- confusionMatrix(rf_test_pred, test_data$Diabetes_binary, positive = "Diabetes")
print(cm_test)

# Extract performance metrics
test_accuracy  <- cm_test$overall["Accuracy"]
test_precision <- cm_test$byClass["Precision"]
test_recall    <- cm_test$byClass["Recall"]
test_f1        <- cm_test$byClass["F1"]

# ROC/AUC on test set
roc_test <- roc(test_data$Diabetes_binary, rf_test_prob)
test_auc <- auc(roc_test)

# Combine CV and Test results for comparison 
results_compare <- data.frame(
  Metric    = c("Accuracy","Precision","Recall","F1","AUC"),
  CV_Avg    = round(cv_summary, 3),
  Test_Set  = round(c(test_accuracy, test_precision, test_recall, test_f1, test_auc), 3)
)

print(results_compare)

plot(roc_test, col = "blue", main = "Random Forest ROC - Test Set")
legend("bottomright", legend = paste("AUC =", round(test_auc, 3)), col = "blue", lwd = 2)

```

