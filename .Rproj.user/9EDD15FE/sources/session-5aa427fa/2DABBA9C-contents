---
title: "final project test"
author: "Meiqi Zhu"
date: "2025-11-13"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(randomForest)
library(caret)
library(dplyr)
library(pROC)
library(ggplot2)
```

```{r}
data <- read.csv("D:/Users/Desktop/BIOSTAT 625/Final_project/Data Base/diabetes/diabetes_binary_5050split.csv")
```

```{r}
head(data)
summary(data)
```
```{r}
set.seed(123)

# Data preparation
data_clean <- data %>%
  mutate(
    Diabetes_binary = as.factor(Diabetes_binary),
    HighBP = as.factor(HighBP),
    HighChol = as.factor(HighChol),
    CholCheck = as.factor(CholCheck),
    Smoker = as.factor(Smoker),
    Stroke = as.factor(Stroke),
    HeartDiseaseorAttack = as.factor(HeartDiseaseorAttack),
    PhysActivity = as.factor(PhysActivity),
    Fruits = as.factor(Fruits),
    Veggies = as.factor(Veggies),
    HvyAlcoholConsump = as.factor(HvyAlcoholConsump),
    AnyHealthcare = as.factor(AnyHealthcare),
    NoDocbcCost = as.factor(NoDocbcCost),
    DiffWalk = as.factor(DiffWalk),
    Sex = as.factor(Sex)
  )

sum(is.na(data_clean))

# Split data into 70% training and 30% testing
train_index <- createDataPartition(data_clean$Diabetes_binary, 
                                  p = 0.7, 
                                  list = FALSE)
train_data <- data_clean[train_index, ]
test_data <- data_clean[-train_index, ]

cat("Training set size:", nrow(train_data), "\n")
cat("Testing set size:", nrow(test_data), "\n")
cat("Training set proportion:", mean(train_data$Diabetes_binary == "1"), "\n")
cat("Testing set proportion:", mean(test_data$Diabetes_binary == "1"), "\n")

rf_model <- randomForest(
  Diabetes_binary ~ .,
  data = train_data,
  ntree = 500,           # Number of trees
  mtry = sqrt(ncol(train_data) - 1),  # Features per split
  importance = TRUE,     # Calculate variable importance
  proximity = FALSE
)

print(rf_model)

rf_predictions <- predict(rf_model, test_data)
rf_probabilities <- predict(rf_model, test_data, type = "prob")[,2]

# confusion matrix
conf_matrix <- confusionMatrix(rf_predictions, test_data$Diabetes_binary, positive = "1")
print(conf_matrix)

accuracy <- conf_matrix$overall['Accuracy']
precision <- conf_matrix$byClass['Precision']
recall <- conf_matrix$byClass['Recall']
f1_score <- conf_matrix$byClass['F1']

cat("\nModel Performance Metrics:\n")
cat("Accuracy:", round(accuracy, 4), "\n")
cat("Precision:", round(precision, 4), "\n")
cat("Recall:", round(recall, 4), "\n")
cat("F1-Score:", round(f1_score, 4), "\n")

# ROC Curve and AUC
roc_result <- roc(test_data$Diabetes_binary, rf_probabilities)
auc_value <- auc(roc_result)

cat("AUC:", round(auc_value, 4), "\n")

# Plot ROC curve
plot(roc_result, main = "ROC Curve - Random Forest")
legend("bottomright", legend = paste("AUC =", round(auc_value, 4)))

# Variable Importance Plot
var_importance <- importance(rf_model)
varImpPlot(rf_model, 
           main = "Variable Importance - Random Forest",
           type = 2)  # Mean decrease in Gini

# Convert to data frame for better visualization
var_imp_df <- data.frame(
  Variable = rownames(var_importance),
  Importance = var_importance[, "MeanDecreaseGini"]
) %>%
  arrange(desc(Importance))

# Plot variable importance using ggplot
ggplot(var_imp_df[1:15, ], aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Top 15 Most Important Variables - Random Forest",
       x = "Variables",
       y = "Mean Decrease in Gini") +
  theme_minimal()

# FIXED: Save important variables for comparison with other methods
# Use base R to avoid dplyr select conflicts
important_vars <- var_imp_df[1:10, c("Variable", "Importance")]
print(important_vars)

# Alternative if you want to use dplyr explicitly:
# important_vars <- var_imp_df %>%
#   dplyr::select(Variable, Importance) %>%
#   head(10)

# Hyperparameter tuning (optional but recommended)
# Tune mtry parameter
tune_result <- tuneRF(
  x = train_data[, -1],  # All columns except Diabetes_binary
  y = train_data$Diabetes_binary,
  ntreeTry = 500,
  stepFactor = 1.5,
  improve = 0.01,
  trace = TRUE,
  plot = TRUE
)

# Get best mtry
if (length(tune_result) > 0) {
  best_mtry <- tune_result[tune_result[, 2] == min(tune_result[, 2]), 1]
  cat("Best mtry:", best_mtry, "\n")
  
  # Train final model with tuned parameters
  rf_model_tuned <- randomForest(
    Diabetes_binary ~ .,
    data = train_data,
    ntree = 500,
    mtry = best_mtry,
    importance = TRUE
  )
  
  # Evaluate tuned model
  rf_predictions_tuned <- predict(rf_model_tuned, test_data)
  conf_matrix_tuned <- confusionMatrix(rf_predictions_tuned, test_data$Diabetes_binary, positive = "1")
  
  cat("\nTuned Model Performance:\n")
  cat("Accuracy:", round(conf_matrix_tuned$overall['Accuracy'], 4), "\n")
  cat("Precision:", round(conf_matrix_tuned$byClass['Precision'], 4), "\n")
  cat("Recall:", round(conf_matrix_tuned$byClass['Recall'], 4), "\n")
  cat("F1-Score:", round(conf_matrix_tuned$byClass['F1'], 4), "\n")
  
  # Compare feature importance between models
  par(mfrow = c(1, 2))
  varImpPlot(rf_model, main = "Original RF - Variable Importance")
  varImpPlot(rf_model_tuned, main = "Tuned RF - Variable Importance")
  
  # Reset plot layout
  par(mfrow = c(1, 1))
  
  final_model <- rf_model_tuned
} else {
  cat("Tuning did not improve model. Using original model.\n")
  final_model <- rf_model
}

# Save model results for group comparison
rf_results <- list(
  model = final_model,
  accuracy = if(exists("conf_matrix_tuned")) conf_matrix_tuned$overall['Accuracy'] else accuracy,
  precision = if(exists("conf_matrix_tuned")) conf_matrix_tuned$byClass['Precision'] else precision,
  recall = if(exists("conf_matrix_tuned")) conf_matrix_tuned$byClass['Recall'] else recall,
  f1_score = if(exists("conf_matrix_tuned")) conf_matrix_tuned$byClass['F1'] else f1_score,
  auc = auc_value,
  important_variables = important_vars
)

cat("\n=== RANDOM FOREST FINAL RESULTS ===\n")
cat("Best Model Accuracy:", round(rf_results$accuracy, 4), "\n")
cat("Precision:", round(rf_results$precision, 4), "\n")
cat("Recall:", round(rf_results$recall, 4), "\n")
cat("F1-Score:", round(rf_results$f1_score, 4), "\n")
cat("AUC:", round(rf_results$auc, 4), "\n")
cat("\nTop 5 Most Important Variables:\n")
print(head(rf_results$important_variables, 5))

write.csv(rf_results$important_variables, "random_forest_important_variables.csv", row.names = FALSE)

results_table <- data.frame(
  Metric = c("Accuracy", "Precision", "Recall", "F1-Score", "AUC"),
  Value = c(
    round(rf_results$accuracy, 4),
    round(rf_results$precision, 4),
    round(rf_results$recall, 4),
    round(rf_results$f1_score, 4),
    round(rf_results$auc, 4)
  )
)

print(results_table)
```
```{r}
# Test interaction between BMI and Physical Activity
interaction_bmi_phys <- glm(Diabetes_binary ~ BMI * PhysActivity, 
                           data = data_clean, family = binomial)
summary(interaction_bmi_phys)

# Test interaction between BMI and General Health
interaction_bmi_genhlth <- glm(Diabetes_binary ~ BMI * GenHlth, 
                              data = data_clean, family = binomial)
summary(interaction_bmi_genhlth)

# Test interaction between BMI and Physical Health
interaction_bmi_physhlth <- glm(Diabetes_binary ~ BMI * PhysHlth, 
                               data = data_clean, family = binomial)
summary(interaction_bmi_physhlth)

# Test interaction between BMI and Difficulty Walking
interaction_bmi_diffwalk <- glm(Diabetes_binary ~ BMI * DiffWalk, 
                               data = data_clean, family = binomial)
summary(interaction_bmi_diffwalk)

# Compare all interactions in one model
full_interaction_model <- glm(Diabetes_binary ~ BMI * (PhysActivity + GenHlth + PhysHlth + DiffWalk),
                             data = data_clean, family = binomial)
summary(full_interaction_model)

# Create interaction plots
library(ggplot2)

# BMI vs Physical Activity interaction plot
ggplot(data_clean, aes(x = BMI, y = as.numeric(Diabetes_binary) - 1, 
                       color = PhysActivity, group = PhysActivity)) +
  geom_smooth(method = "glm", method.args = list(family = "binomial"), 
              se = TRUE) +
  labs(title = "BMI and Physical Activity Interaction",
       x = "BMI", y = "Probability of Diabetes",
       color = "Physical Activity") +
  theme_minimal()

# BMI vs General Health interaction plot
ggplot(data_clean, aes(x = BMI, y = as.numeric(Diabetes_binary) - 1, 
                       color = as.factor(GenHlth), group = as.factor(GenHlth))) +
  geom_smooth(method = "glm", method.args = list(family = "binomial"), 
              se = TRUE) +
  labs(title = "BMI and General Health Interaction",
       x = "BMI", y = "Probability of Diabetes",
       color = "General Health") +
  theme_minimal()

# Create BMI categories for easier interpretation
data_clean <- data_clean %>%
  mutate(BMI_category = case_when(
    BMI < 18.5 ~ "Underweight",
    BMI >= 18.5 & BMI < 25 ~ "Normal",
    BMI >= 25 & BMI < 30 ~ "Overweight", 
    BMI >= 30 ~ "Obese"
  ))

# Cross-tabulation: BMI category vs Physical Activity
table(data_clean$BMI_category, data_clean$PhysActivity)
prop.table(table(data_clean$BMI_category, data_clean$PhysActivity), 1)

# Diabetes rates by BMI category and Physical Activity
diabetes_rates <- data_clean %>%
  group_by(BMI_category, PhysActivity) %>%
  summarize(
    n = n(),
    diabetes_rate = mean(as.numeric(Diabetes_binary) - 1),
    .groups = 'drop'
  )

print(diabetes_rates)

# Plot the results
ggplot(diabetes_rates, aes(x = BMI_category, y = diabetes_rate, 
                          fill = as.factor(PhysActivity))) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Diabetes Rates by BMI Category and Physical Activity",
       x = "BMI Category", y = "Diabetes Rate",
       fill = "Physical Activity") +
  theme_minimal() +
  scale_fill_manual(values = c("0" = "red", "1" = "blue"),
                    labels = c("No Activity", "Active"))

```
